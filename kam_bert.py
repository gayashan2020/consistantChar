# -*- coding: utf-8 -*-
"""KAM-BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cxfmhnEr7rAmLO-hy6HIiEAnDCs81VAO
"""

!pip install datasets
from datasets import load_dataset

!pip install transformers

!pip install optimum

!pip install auto-gptq

import requests

url = "https://huggingface.co/datasets/Akila/ForgottenRealmsWikiDataset/raw/main/FRW-FJ.json"
response = requests.get(url)
with open('FRW-FJ.json', 'wb') as file:
    file.write(response.content)

import json

with open('FRW-FJ.json', 'r') as file:
    data = json.load(file)

import torch
from transformers import BertTokenizer, BertForSequenceClassification, BertModel

from transformers import AutoTokenizer, AutoModelForCausalLM

!pip uninstall auto-gptq -y
!pip uninstall optimum -y
!pip install auto-gptq
!pip install optimum

import optimum
import auto_gptq

tokenizer = AutoTokenizer.from_pretrained("TheBloke/OpenOrca-Platypus2-13B-GPTQ")
model = AutoModelForCausalLM.from_pretrained("TheBloke/OpenOrca-Platypus2-13B-GPTQ")

# Check if GPU is available
if torch.cuda.is_available():
    device = torch.device('cuda')
    print('Using GPU:', torch.cuda.get_device_name(0))
else:
    device = torch.device('cpu')
    print('Using CPU')


model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)
model.to(device)

knowledge_base = {}
for entry in data:
    page = entry['page']
    content = entry['content']
    knowledge_base[page] = content


def generate_kam(sentence, knowledge_base):
    tokens = tokenizer.tokenize(sentence)
    attention_map = []
    for token in tokens:
        # Retrieve related knowledge for each token from the knowledge_base
        knowledge = knowledge_base.get(token, "")
        # Generate attention scores based on the relevance of the knowledge to the token
        score = len(knowledge)
        attention_map.append(score)

    # Normalize the attention scores
    total_score = sum(attention_map)
    if total_score == 0:
        # If total_score is zero, assign equal attention to all tokens
        attention_map = [1/len(tokens) for _ in tokens]
    else:
        attention_map = [score / total_score for score in attention_map]

    return attention_map

with open('/content/Whispering.txt', 'r', encoding='ISO-8859-1') as file:
    document = file.read()



def segment_text(text, model, tokenizer):
    # Use the Alpaca-InstructOnly prompt template
    input_text = f"### Instruction:\n\nSegment the following text into meaningful sections: {text}\n\n### Response:"
    input_ids = tokenizer.encode(input_text, return_tensors="pt")
    output = model.generate(input_ids, max_length=1000, num_return_sequences=1, no_repeat_ngram_size=2)
    segmented_text = tokenizer.decode(output[0], skip_special_tokens=True)

    # Extract the segmented sections from the model's response
    # We'll extract the content after "### Response:"
    sections = segmented_text.split("### Response:")[-1].strip().split("\n\n")
    return sections

sections = segment_text(document, model, tokenizer)
# sections
summarized_paragraphs = []
for paragraph in paragraphs:
    tokens = tokenizer.tokenize(paragraph)
    attention_map = generate_kam(paragraph, knowledge_base)
    # For this implementation, we'll just return the original paragraph
    summarized_paragraphs.append(paragraph)

entities = [word for word in document.split() if word.istitle()]

prompts = []
for paragraph, entity in zip(summarized_paragraphs, entities):
    prompt = f'A scenic view of the {entity} described as: {paragraph}'
    prompts.append(prompt)

for prompt in prompts:
    print(prompt)