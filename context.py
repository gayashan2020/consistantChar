# -*- coding: utf-8 -*-
"""context.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19hhCGgmxqI1Ik-o3HHRBwadUcwnbEetZ
"""

!pip install transformers

!pip install flair

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from gensim import corpora, models
import numpy as np
from flair.data import Sentence
from flair.models import SequenceTagger

# Load your data from data.csv
data = pd.read_csv('convertcsv.csv')

# Assuming data has columns: title, content

# Step 2: Context Identification

# Named Entity Recognition using Flair
tagger = SequenceTagger.load("flair/ner-english-ontonotes")

def get_entities_flair(text):
    sentence = Sentence(text)
    tagger.predict(sentence)
    entities = sentence.get_spans('ner')
    return [entity.text for entity in entities]

data['entities'] = data['content'].apply(get_entities_flair)

# Topic Modeling
texts = data['content'].str.split()
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
lda_model = models.LdaModel(corpus, num_topics=4, id2word=dictionary, passes=15)

# Text Classification
def classify_section(title):
    return "Introduction" if "introduction" in title.lower() else "Other"

data['section_type'] = data['title'].apply(classify_section)

# Keyword Extraction
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(data['content'])
feature_array = vectorizer.get_feature_names_out()
tfidf_sorting = [np.argsort(x.toarray()).flatten()[::-1] for x in X]
top_n = 10
data['top_keywords'] = [list(feature_array[tfidf_sorting][indices][:top_n]) for indices in range(len(tfidf_sorting))]

# Table Identification (Assuming a simplistic heuristic for identifying tables based on the presence of tab characters)
def contains_table(text):
    return '\t' in text

data['contains_table'] = data['content'].apply(contains_table)

# Output the results
data.to_csv('context_identified_data_2.csv', index=False)