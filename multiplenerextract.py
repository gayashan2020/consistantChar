# -*- coding: utf-8 -*-
"""multipleNerExtract.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m_p_nK4x3S6uBDpfSQ8N5kowKF4NA5_S
"""

!pip install transformers
# !pip install flair
!pip install https://huggingface.co/spacy/en_core_web_sm/resolve/main/en_core_web_sm-any-py3-none-any.whl
!pip install sentencepiece
# !pip install trankit
# import spacy
from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline
# from flair.data import Sentence
# from flair.models import SequenceTagger
# from trankit import Pipeline

listOfFiles = ['hoard.txt','Tiamat.txt','CURSEOFSTRAHD.txt','CandlekeepMysteries.txt','LOSTMINEOFPHANDELVER.txt','THEWILDBEYONDTHEWITCHLIGHT.txt','TOMBOFANNIHILATION.txt']
# listOfFiles = ['CURSEOFSTRAHD.txt']

listOfText = []

def splitter(n, s):
    pieces = s.split()
    return (" ".join(pieces[i:i+n]) for i in range(0, len(pieces), n))


for item in listOfFiles:
  with open(item, encoding="unicode_escape") as f:
    text = f.read()
    # temp = []
    # for piece in splitter(500000, text):
    #   temp.append(piece)
    listOfText.append(text)

list_of_tokenizers = ["xlm-roberta-large-finetuned-conll03-english","StanfordAIMI/stanford-deidentifier-base","dbmdz/electra-large-discriminator-finetuned-conll03-english","Babelscape/wikineural-multilingual-ner","Davlan/bert-base-multilingual-cased-ner-hrl","Jean-Baptiste/roberta-large-ner-english","dslim/bert-base-NER"]

def use_pipelines(list_of_tokenizers,listOflists):
  tokenizer = AutoTokenizer.from_pretrained(list_of_tokenizers)
  model = AutoModelForTokenClassification.from_pretrained(list_of_tokenizers)

  nlp = pipeline("ner", model=model, tokenizer=tokenizer)
  all = []
  i=0
  for para in listOflists:
    # for section in para:
    inter = []
    ner_results = nlp(para)
    with open(str(i)+list_of_tokenizers[0]+list_of_tokenizers[1]+'.txt', 'w') as f:
      f.write(str(ner_results))
    # for entity in ner_results:
    #   inter.append(entity['word'])
    # all.append(ner_results)
    i+=1

  # return all

allResults = []
for token in list_of_tokenizers:
    print('-------------------------',token,'-------------------------')
    result = use_pipelines(token,listOfText)
    # allResults.append(result)

def splitText(textVal):
  returnVal = []
  for k in textVal:
    returnVal.append(k.split('.')[0])
  return returnVal

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

!sudo pip install xlsxwriter

# len(allResults[0][1])
for i in range(7):
  print(len(allResults[7][i]))

# len(allResults[0])
import pandas as pd
writer = pd.ExcelWriter('output.xlsx', engine='xlsxwriter')
j=1
for item in allResults:
  df = pd.DataFrame(list(zip(item[0], item[1], item[2], item[3], item[4], item[5], item[6])),
               columns =splitText(listOfFiles))
  df.to_excel(writer, sheet_name=list_of_tokenizers[j-1].split('-')[0].split('/')[0])
  j+=1

writer.close()

tagger = SequenceTagger.load("flair/ner-english")

for para in listOfText:
  listOfFlair = []
  # for item in para:
  sentence = Sentence(para)
  # tempList = []
  tagger.predict(sentence)
  for entity in sentence.get_spans('ner'):
    listOfFlair.append(entity)
  # listOfFlair.append(tempList)

listOfFlair

